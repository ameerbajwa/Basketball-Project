{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import requests, re\n",
    "import random\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of abbreviated teams from basketball-reference.com\n",
    "\n",
    "teams = ['ATL', 'BOS', 'NJN', 'CHA', 'CHI', 'CLE', 'DAL', 'DEN', 'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA',\n",
    "        'MIL', 'MIN', 'NOH', 'NYK', 'OKC', 'ORL', 'PHI', 'PHO', 'POR', 'SAC', 'SAS', 'TOR', 'UTA', 'WAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping basketball-reference.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting URLs of each NBA team first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to scrape urls of NBA teams\n",
    "\n",
    "us = UserAgent()\n",
    "user_agent = {'User-Agent':us.random}\n",
    "\n",
    "def get_team_info(team):\n",
    "    url = 'https://www.basketball-reference.com/teams/'+team+'/'\n",
    "    response = requests.get(url, headers=user_agent)\n",
    "    page = response.text\n",
    "    print (url)\n",
    "    time.sleep(1)\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scraping basketball-reference.com for their NBA teams\n",
    "\n",
    "team_by_year_info = {}\n",
    "\n",
    "for team in teams:\n",
    "    soup = BeautifulSoup(get_team_info(team), 'lxml')\n",
    "    team_by_year_info[team] = soup\n",
    "\n",
    "print (len(team_by_year_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting URLs of each NBA team's season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From the soup objects of each NBA team, scraping the urls of each season for each NBA team\n",
    "\n",
    "real_team_player_info = {}\n",
    "\n",
    "for team, soup in team_by_year_info.items():\n",
    "    teams_info = soup.table.tbody\n",
    "    \n",
    "    rows = teams_info.find_all('tr')\n",
    "    \n",
    "    for row in rows[1:]:\n",
    "        year = row.find('th').find('a').text[0:4]\n",
    "        year = int(year)+1\n",
    "        final_year = str(year)\n",
    "        \n",
    "        url = 'https://www.basketball-reference.com/teams/'+team+'/'+final_year+'.html'\n",
    "        response = requests.get(url, headers=user_agent)\n",
    "        page = response.text\n",
    "        \n",
    "        real_team_player_info[team + '_' + final_year] = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "        print (url)\n",
    "        time.sleep(random.randint(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting each player's info and stats from each soup object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_player_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scraping all the urls of each NBA team's season to get all the players' statistics\n",
    "#   information\n",
    "\n",
    "for team_year, info in real_team_player_info.items():\n",
    "\n",
    "# Use this if statement since some of the websites I scraped don't have a page, so this is \n",
    "#   a precautionary measure to only grab all the information when the page provides it\n",
    "\n",
    "    if len(info.find('div', {'id':'content'}).find_all('div')) > 2:\n",
    "\n",
    "        team_year_player_info = {}\n",
    "    \n",
    "        players = info.table\n",
    "        player_info = [row for row in players.find_all('tr')]\n",
    "        for i in player_info[1:]:\n",
    "            player_name = i.find('td').text\n",
    "            info_on_player = i.find_all('td')\n",
    "\n",
    "            team_year_player_info[team_year+'_'+player_name] = [i.text for i in info_on_player if i.text != player_name]\n",
    "\n",
    "# Lots of the statistics information on each page are commented out so I have to use the\n",
    "#    Comment from the package bs4 to be able to grab all the season, advanced, shooting,\n",
    "#    and salary stats for each player\n",
    "\n",
    "        comments = info.find_all(text=lambda text:isinstance(text, Comment))\n",
    "\n",
    "        rx_stat = re.compile(r'<table.+?id=\"per_game\".+?>[\\s\\S]+?</table>')\n",
    "        rx_adv = re.compile(r'<table.+?id=\"advanced\".+?[\\s\\S]+?</table>')\n",
    "        rx_shot = re.compile(r'<table.+?id=\"shooting\".+?>[\\s\\S]+?</table>')\n",
    "        rx_sal = re.compile(r'<table.+?id=\"salaries2\".+?>[\\s\\S]+?</table>')\n",
    "\n",
    "# Using try and except statements for each commented table I want to grab because some pages\n",
    "#    don't have every statistic table I want\n",
    "\n",
    "        for comment in comments:\n",
    "            try:\n",
    "                stat_table = rx_stat.search(comment.string).group(0)\n",
    "                stats_table = BeautifulSoup(stat_table, 'lxml')\n",
    "                stats_info = [p for p in stats_table.find_all('tr')]\n",
    "\n",
    "                for i in stats_info[1:]:\n",
    "                    player_name_stats = i.find('td').text\n",
    "                    stats = i.find_all('td')\n",
    "                    real_stats = [rs.text for rs in stats]\n",
    "            \n",
    "                    for key, value in team_year_player_info.items():\n",
    "                        if player_name_stats in key:\n",
    "                            team_year_player_info[key].extend(real_stats[1:])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                adv_table = rx_adv.search(comment.string).group(0)\n",
    "                ad_table = BeautifulSoup(adv_table, 'lxml')\n",
    "                adv_info = [a for a in ad_table.find_all('tr')]\n",
    "\n",
    "                for i in adv_info[1:]:\n",
    "                    player_name_adv = i.find('td').text\n",
    "                    adv = i.find_all('td')\n",
    "                    real_adv = [ad.text for ad in adv]\n",
    "\n",
    "                    for key, value in team_year_player_info.items():\n",
    "                        if player_name_adv in key:\n",
    "                            team_year_player_info[key].extend(real_adv[1:])       \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                shot_table = rx_shot.search(comment.string).group(0)\n",
    "                sh_table = BeautifulSoup(shot_table, 'lxml')\n",
    "                shot_info = [sh for sh in sh_table.find_all('tr')]\n",
    "\n",
    "                for i in shot_info[3:]:\n",
    "                    player_name_shot = i.find('td').text\n",
    "                    shooting = i.find_all('td')\n",
    "                    real_shooting = [sh.text for sh in shooting]\n",
    "     \n",
    "                    for key, value in team_year_player_info.items():\n",
    "                        if player_name_shot in key:\n",
    "                            team_year_player_info[key].extend(real_shooting[1:])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                sal_table = rx_sal.search(comment.string).group(0)\n",
    "                salary_table = BeautifulSoup(sal_table, 'lxml')\n",
    "                salary_info = [s for s in salary_table.find_all('tr')]\n",
    "\n",
    "                for i in salary_info[1:]:\n",
    "                    player_name_sal = i.find('td').text\n",
    "                    salary = i.find('td').nextSibling.text\n",
    "\n",
    "                    for key, value in team_year_player_info.items():\n",
    "                        if player_name_sal in key:\n",
    "                            team_year_player_info[key].append(salary)               \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Uploading each roster of players' statistics into a list\n",
    "\n",
    "        full_player_info.append(team_year_player_info)\n",
    "    \n",
    "        print (team_year+'_'+player_name)\n",
    "    else:\n",
    "        print (team_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Organizing the full_player_info list into a complete dictionary of a player/his season/his team and his stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uploading the list of NBA team's rosters from each season players' statistics into one giant\n",
    "#    dictionary to easily look through each player's stats\n",
    "\n",
    "final_player_info = {}\n",
    "for player in full_player_info:\n",
    "    final_player_info.update(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this for loop to add on any \"Not Applicable\" values to NBA players whose stats don't\n",
    "#    add up to 84. I add these NA values because some of the webpages don't have all of the\n",
    "#    statistic tables, so some players will have less than the 84 total statistic values.\n",
    "\n",
    "for key, value in final_player_info.items():\n",
    "    i = len(value)\n",
    "    if i < 84:\n",
    "        while i < 84:\n",
    "            value.append('NA')\n",
    "            i += 1\n",
    "    if i > 84:\n",
    "        del value[84:]\n",
    "    final_player_info[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I switch the salary in the list of each dictionary value to the last value on the list if \n",
    "#    it isn't already\n",
    "\n",
    "for key, value in final_player_info.items():\n",
    "    for i in value:\n",
    "        if '$' in i:\n",
    "            salary_index = value.index(i)\n",
    "        else:\n",
    "            salary_index = 'no_money_info'\n",
    "    if type(salary_index) == int:\n",
    "        value[salary_index], value[-1] = value[-1], value[salary_index]\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting rid of the '$' and ',' in the salary values in every player's stats\n",
    "\n",
    "for key, value in final_player_info.items():\n",
    "    if '$' in value[-1]:\n",
    "        value[-1] = ''.join([c for c in value[-1] if c not in ('$', ',')])\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the final_player_info into a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dumping the large dictionary file into a pickle object so I don't have to scrape basketball-reference.com\n",
    "#    multiple times.\n",
    "\n",
    "with open('saved_final_player_info.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_player_info, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "### Converting player_stats dictionary into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Names of each column\n",
    "\n",
    "names = ['position', 'height', 'weight', 'D_of_B', 'Country', 'Exp', 'College', 'Age', 'games_played', 'games_started',\n",
    "        'min_played/G', 'fg_made/G', 'fg_att/G', 'fg%', '3PM/G', '3PA/G', '3P%', '2PM/G', '2PA/G', '2P%', 'eFG%', 'FTM/G',\n",
    "         'FTA/G', 'FT%','ORB/G', 'DRB/G', 'TRB/G', 'AST/G', 'STL/G', 'BLK/G', 'TOV/G', 'PF/G', 'PTS/G', 'Age2', 'games_played2',\n",
    "        'min_played_total', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'random1',\n",
    "        'OWS', 'DWS', 'WS', 'WS/48', 'random2', 'OBPM', 'DBPM', 'BPM', 'VORP', 'Age3', 'games_played3', 'min_played_total2', 'FG%2', 'Dist', '%_shots_2PA',\n",
    "        '2PA%_0-3', '2PA%_3-10', '2PA%_10-16', '2PA%_16<3', '3PA%', '%_shots_2PM', '2PM%_0-3', '2PM%_3-10', '2PM%_10-16',\n",
    "        '2PM%_16<3', '3PM%', '%Astd_forFGM', '%_FGA_dunks', 'dunks', '%Astd_for3FGM', '%_3PA_corner', '3PM%_corner',\n",
    "        'heaves_att', 'heaves_made', 'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking to see if loading the dictionary into a pandas dataframe works\n",
    "\n",
    "player_df = pd.DataFrame(final_player_info, index=names)\n",
    "player_df_original = player_df.T\n",
    "player_df_original"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
